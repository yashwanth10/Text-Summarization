{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!git clone https://github.com/AI4Bharat/IndicTrans2.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%cd /content/IndicTrans2/huggingface_interface","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n!python3 -c \"import nltk; nltk.download('punkt')\"\n!python3 -m pip install bitsandbytes scipy accelerate datasets\"\n!python3 -m pip install sentencepiece\n\n!git clone https://github.com/VarunGumma/IndicTransTokenizer\n%cd IndicTransTokenizer\n!python3 -m pip install --editable ./\n%cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now Restart the session and execute from now onwards**","metadata":{}},{"cell_type":"code","source":"pip install dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the CNN/DailyMail dataset\ndataset = load_dataset(\"cnn_dailymail\",'1.0.0')\n\n# Get the pandas DataFrame from the dataset\ndf = dataset['train'].to_pandas()\n\n# Select first 5000 rows and save to CSV\nfirst_3000_rows = df.head(3000)\nfirst_3000_rows.to_csv('first_3000_cnn_dailymail.csv', index=False)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:09:28.088970Z","iopub.execute_input":"2024-04-22T21:09:28.089547Z","iopub.status.idle":"2024-04-22T21:10:00.561302Z","shell.execute_reply.started":"2024-04-22T21:09:28.089502Z","shell.execute_reply":"2024-04-22T21:10:00.559620Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32033b792d3f413a90ad67bb4ddd6052"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 256M/256M [00:01<00:00, 206MB/s]  \nDownloading data: 100%|██████████| 257M/257M [00:01<00:00, 205MB/s]  \nDownloading data: 100%|██████████| 259M/259M [00:01<00:00, 256MB/s]  \nDownloading data: 100%|██████████| 34.7M/34.7M [00:00<00:00, 129MB/s] \nDownloading data: 100%|██████████| 30.0M/30.0M [00:00<00:00, 75.8MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a71aa01f765546b3801bfb562f86a200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3347306cd744edadda3e72acb2e7e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05f0baac169641578539e4dea45ebfb5"}},"metadata":{}}]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:31:45.264367Z","iopub.execute_input":"2024-04-21T08:31:45.264763Z","iopub.status.idle":"2024-04-21T08:31:45.283287Z","shell.execute_reply.started":"2024-04-21T08:31:45.264731Z","shell.execute_reply":"2024-04-21T08:31:45.281904Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                             article  \\\n0  LONDON, England (Reuters) -- Harry Potter star...   \n1  Editor's note: In our Behind the Scenes series...   \n2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n3  WASHINGTON (CNN) -- Doctors removed five small...   \n4  (CNN)  -- The National Football League has ind...   \n\n                                          highlights  \\\n0  Harry Potter star Daniel Radcliffe gets £20M f...   \n1  Mentally ill inmates in Miami are housed on th...   \n2  NEW: \"I thought I was going to die,\" driver sa...   \n3  Five small polyps found during procedure; \"non...   \n4  NEW: NFL chief, Atlanta Falcons owner critical...   \n\n                                         id  \n0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  \n2  06352019a19ae31e527f37f7571c6dd7f0c5da37  \n3  24521a2abb2e1f5e34e6824e0f9e56904a2b0e88  \n4  7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>highlights</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Editor's note: In our Behind the Scenes series...</td>\n      <td>Mentally ill inmates in Miami are housed on th...</td>\n      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n      <td>06352019a19ae31e527f37f7571c6dd7f0c5da37</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n      <td>Five small polyps found during procedure; \"non...</td>\n      <td>24521a2abb2e1f5e34e6824e0f9e56904a2b0e88</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(CNN)  -- The National Football League has ind...</td>\n      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n      <td>7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\nfrom IndicTransTokenizer import IndicProcessor, IndicTransTokenizer\n\nBATCH_SIZE = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquantization = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_model_and_tokenizer(ckpt_dir, direction, quantization):\n    if quantization == \"4-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n    elif quantization == \"8-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_8bit=True,\n            bnb_8bit_use_double_quant=True,\n            bnb_8bit_compute_dtype=torch.bfloat16,\n        )\n    else:\n        qconfig = None\n\n    tokenizer = IndicTransTokenizer(direction=direction)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        ckpt_dir,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        quantization_config=qconfig,\n    )\n\n    if qconfig == None:\n        model = model.to(DEVICE)\n        if DEVICE == \"cuda\":\n            model.half()\n\n    model.eval()\n\n    return tokenizer, model\n\n\ndef batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n    translations = []\n    for i in range(0, len(input_sentences), BATCH_SIZE):\n        batch = input_sentences[i : i + BATCH_SIZE]\n\n        # Preprocess the batch and extract entity mappings\n        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n\n        # Tokenize the batch and generate input encodings\n        inputs = tokenizer(\n            batch,\n            src=True,\n            truncation=True,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n        ).to(DEVICE)\n\n        # Generate translations using the model\n        with torch.no_grad():\n            generated_tokens = model.generate(\n                **inputs,\n                use_cache=True,\n                min_length=0,\n                max_length=256,\n                num_beams=5,\n                num_return_sequences=1,\n            )\n\n        # Decode the generated tokens into text\n        generated_tokens = tokenizer.batch_decode(generated_tokens.detach().cpu().tolist(), src=False)\n\n        # Postprocess the translations, including entity replacement\n        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n\n        del inputs\n        torch.cuda.empty_cache()\n\n    return translations","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # ai4bharat/indictrans2-en-indic-dist-200M\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, \"en-indic\", quantization)\n\nip = IndicProcessor(inference=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src_lang, tgt_lang = \"eng_Latn\", \"hin_Deva\"\ndf['article_hindi'] = batch_translate(df['article'], src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\ndf['highlights_hindi'] = batch_translate(df['highlights'], src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# Convert translated_data to DataFrame\ndf_translated = pd.DataFrame(df)\n\n# Save the translated DataFrame to a CSV file\ndf_translated.to_csv('CNN_3000 translated_Hindi_IndicTrans_main.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}